************************************************************************************************************************************************************
**** 配置属性 ：环境变量
************************************************************************************************************************************************************
**** 
************************************************************************************************************************************************************
**** spark-env.sh.template  --> spark-env.sh
************************************************************************************************************************************************************
# Options read when launching programs locally with ./bin/run-example or ./bin/spark-submit
# - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files
# - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node
# - SPARK_PUBLIC_DNS, to set the public dns name of the driver program
# - SPARK_CLASSPATH, default classpath entries to append

# Options read by executors and drivers running inside the cluster
# - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node
# - SPARK_PUBLIC_DNS, to set the public DNS name of the driver program
# - SPARK_CLASSPATH, default classpath entries to append
# - SPARK_LOCAL_DIRS, storage directories to use on this node for shuffle and RDD data
# - MESOS_NATIVE_LIBRARY, to point to your libmesos.so if you use Mesos

# Options read in YARN client mode
# - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files
# - SPARK_EXECUTOR_INSTANCES, Number of workers to start (Default: 2)
# - SPARK_EXECUTOR_CORES, Number of cores for the workers (Default: 1).
# - SPARK_EXECUTOR_MEMORY, Memory per Worker (e.g. 1000M, 2G) (Default: 1G)
# - SPARK_DRIVER_MEMORY, Memory for Master (e.g. 1000M, 2G) (Default: 512 Mb)
# - SPARK_YARN_APP_NAME, The name of your application (Default: Spark)
# - SPARK_YARN_QUEUE, The hadoop queue to use for allocation requests (Default: ‘default’)
# - SPARK_YARN_DIST_FILES, Comma separated list of files to be distributed with the job.
# - SPARK_YARN_DIST_ARCHIVES, Comma separated list of archives to be distributed with the job.

# Options for the daemons used in the standalone deploy mode:
# - SPARK_MASTER_IP, to bind the master to a different IP address or hostname
# - SPARK_MASTER_PORT / SPARK_MASTER_WEBUI_PORT, to use non-default ports for the master
# - SPARK_MASTER_OPTS, to set config properties only for the master (e.g. "-Dx=y")
# - SPARK_WORKER_CORES, to set the number of cores to use on this machine
# - SPARK_WORKER_MEMORY, to set how much total memory workers have to give executors (e.g. 1000m, 2g)
# - SPARK_WORKER_PORT / SPARK_WORKER_WEBUI_PORT, to use non-default ports for the worker
# - SPARK_WORKER_INSTANCES, to set the number of worker processes per node
# - SPARK_WORKER_DIR, to set the working directory of worker processes
# - SPARK_WORKER_OPTS, to set config properties only for the worker (e.g. "-Dx=y")
# - SPARK_HISTORY_OPTS, to set config properties only for the history server (e.g. "-Dx=y")
# - SPARK_DAEMON_JAVA_OPTS, to set config properties for all daemons (e.g. "-Dx=y")
# - SPARK_PUBLIC_DNS, to set the public dns name of the master or workers




************************************************************************************************************************************************************
**** Spark 环境变量
************************************************************************************************************************************************************
# - SPARK_EXAMPLES_JAR=$SPARK_HOME/examples/target/scala-2.9.3/spark-examples_2.9.3-0.7.3.jar  ： 指定 examples 的路径
# - SPARK_SSH_OPTS="-p58422 -o StrictHostKeyChecking=no" ：设定端口以及....



# - LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib
# - SPARK_LIBRARY_PATH=/usr/local/hadoop/hadoop-release/lib/native/Linux-amd64-64




************************************************************************************************************************************************************
**** 其他环境变量
************************************************************************************************************************************************************
# - JAVA_HOME
# - SCALA_HOME






************************************************************************************************************************************************************
**** 
************************************************************************************************************************************************************








************************************************************************************************************************************************************
**** 源码阅读
************************************************************************************************************************************************************
**** SparkContext
"spark.driver.allowMultipleContexts", false				// If true, log warnings instead of throwing exceptions when multiple SparkContexts are active
"spark.master" 																		// "A master URL must be set in your configuration"
"spark.app.name" 																	// "An application name must be set in your configuration"
"spark.logConf" 																	// Log Spark configuration
"spark.driver.host", Utils.localHostName()
"spark.driver.port", "0"
"spark.jars"
"spark.files"
"spark.eventLog.enabled", false
"spark.eventLog.dir", EventLoggingListener.DEFAULT_LOG_DIR = "/tmp/spark-events"



内部设置：在代码中set，没有get
"spark.tachyonStore.folderName",  randomUUID   	// Generate the random name for a temp folder in Tachyon
  		 																					// Add a timestamp as the suffix here to make it more safe
      																					
"SPARK_YARN_MODE"
"spark.executor.id", "driver"



**** Broadcast : Manager、Factory
** spark.broadcast.compress	true	Whether to compress broadcast variables before sending them. Generally a good idea.
** spark.broadcast.factory	org.apache.spark.broadcast.TorrentBroadcastFactory	Which broadcast implementation to use.
** spark.broadcast.blockSize	4096	Size of each piece of a block in kilobytes for TorrentBroadcastFactory. Too large a value decreases parallelism during broadcast (makes it slower); however, if it is too small, BlockManager might take a performance hit.
** spark.broadcast.port	(random)	Port for the driver's HTTP broadcast server to listen on. This is not relevant for torrent broadcast.

**** HttpBroadcast
** "spark.buffer.size", 65536 
** "spark.broadcast.compress", true
** "spark.httpBroadcast.uri",  serverUri ―― 内部设置：构建 HttpServer 时



**** BlockManager
**** MemoryStore
** spark.storage.unrollFraction	0.2	Fraction of spark.storage.memoryFraction to use for unrolling blocks in memory. This is dynamically allocated by dropping existing blocks when there is not enough free storage space to unroll the new block in its entirety.
** "spark.storage.unrollMemoryThreshold", 1024 * 1024  Initial memory to request before unrolling any block
** 
** 
** 
** 
** 
** 
** 
** 




